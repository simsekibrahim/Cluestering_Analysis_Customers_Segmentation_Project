# -*- coding: utf-8 -*-
"""Cluestering_Analysis_Customers_Segmentation_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h6ZycOzqarwMNRgLgBCFXUUMj2O_ozF6

___

<p style="text-align: center;"><img src="https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV" class="img-fluid" alt="CLRSWY"></p>

___

# WELCOME!

Welcome to "***Clustering (Customer Segmentation) Project***". This is the last medium project of ***Machine Learning*** course. 

At the end of this project, you will have performed ***Cluster Analysis*** with an ***Unsupervised Learning*** method.

---

In this project, customers are required to be segmented according to the purchasing history obtained from the membership cards of a big mall.

This project is less challenging than other projects. After getting to know the data set quickly, you are expected to perform ***Exploratory Data Analysis***. You should observe the distribution of customers according to different variables, also discover relationships and correlations between variables. Then you will spesify the different variables to use for cluster analysis.

Finally, you should clustered customers using the ***K-Means Clustering*** method, after that label the clusters.

- ***NOTE:*** *This project assumes that you already know the basics of coding in Python. You should also be familiar with the theory behind Cluster Analysis and scikit-learn module as well as Machine Learning before you begin.*

---
---

# #Tasks

Mentoring Prep. and self study#### 

#### 1. Import Libraries, Load Dataset, Exploring Data
- Import Libraries
- Load Dataset
- Explore Data

#### 2. Exploratory Data Analysis (EDA)


#### 3. Cluster Analysis

- Clustering based on Age and Spending Score

    *i. Create a new dataset with two variables of your choice*
    
    *ii. Determine optimal number of clusters*
    
    *iii. Apply K Means*
    
    *iv. Visualizing and Labeling All the Clusters*
    
    
- Clustering based on Annual Income and Spending Score

    *i. Create a new dataset with two variables of your choice*
    
    *ii. Determine optimal number of clusters*
    
    *iii. Apply K Means*
    
    *iv. Visualizing and Labeling All the Clusters*
    
    
- Hierarchical Clustering

    *i. Determine optimal number of clusters using Dendogram*

    *ii. Apply Agglomerative Clustering*

    *iii. Visualizing and Labeling All the Clusters* 

- Conclusion

---
---

## 1. Import Libraries, Load Dataset, Exploring Data

There is a big mall in a specific city that keeps information of its customers who subscribe to a membership card. In the membetrship card they provide following information : gender, age and annula income. The customers use this membership card to make all the purchases in the mall, so tha mall has the purchase history of all subscribed members and according to that they compute the spending score of all customers. You have to segment these customers based on the details given.

#### Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
# %matplotlib inline

#plt.rcParams["figure.figsize"] = (10,6)
import warnings
warnings.filterwarnings('ignore')
#pd.set_option('display.max_rows', 500)

import plotly.express as px
import cufflinks as cf
#Enabling the offline mode for interactive plotting locally
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot

init_notebook_mode(connected=True)
cf.go_offline()

#To display the plots
# %matplotlib inline
from ipywidgets import interact
import plotly.io as pio

pio.renderers.default = 'notebook'

import warnings
warnings.filterwarnings("ignore")

pd.set_option("display.precision", 3)
pd.options.display.float_format = '{:,.2f}'.format
from scipy.cluster.hierarchy import dendrogram, linkage

"""#### Load Dataset"""

df = pd.read_csv("Mall_Customers.csv")
df.head()

df.tail()

df.sample(10)

"""#### Explore Data

You can rename columns to more usable, if you need.
"""

df.info()

df.shape

df.describe()

df = df.drop('CustomerID', axis=1)

df.sample(5)

"""---
---

## 2. Exploratory Data Analysis (EDA)

After performing Cluster Analysis, you need to know the data well in order to label the observations correctly. Analyze frequency distributions of features, relationships and correlations between the independent variables and the dependent variable. It is recommended to apply data visualization techniques. Observing breakpoints helps you to internalize the data.
"""

df.nunique().sort_values()

df.isnull().sum().sort_values(ascending=False).head()

df.Gender.value_counts()

df['Gender'] = df['Gender'].replace(['Female', 'Male'], ['0', '1'])

df['Gender'] = df['Gender'].astype(int)

df.Gender.value_counts()

df.info()

sns.countplot(df['Gender']);

sns.distplot(df['Age'])

sns.distplot(df['Annual Income (k$)'])

sns.catplot(x="Gender", y="Age", kind="box", data=df);

sns.pairplot(df)
plt.show()



"""## 3. Cluster Analysis

The main purpose of this project is to perform [cluster analysis](https://en.wikipedia.org/wiki/Cluster_analysis#:~:text=Cluster%20analysis%20or%20clustering%20is,in%20other%20groups%20(clusters).) with the [K-Means](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1) algorithm. 

You can perform many [cluster analysis](http://www.stat.columbia.edu/~madigan/W2025/notes/clustering.pdf) using different variables. If you use a maximum of two variables for each cluster analysis, you can identify cluster labels more clearly.

First, the K-Means algorithm expects you to determine the number of clusters (*n_clusters*). You can determine the optimal number of clusters for each cluster analysis in various ways. In this case, you are expected to use the [Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering).

Finally, different information is obtained in each analysis. Therefore, different labeling should be done as a result of each cluster analysis. 

Labeling example: 

- **Normal Customers**  -- An Average consumer in terms of spending and Annual Income
- **Spender Customers** --  Annual Income is less but spending high, so can also be treated as potential target customer.

### Clustering based on Age and Spending Score

#### *i. Create a new dataset with two variables of your choice*
"""

df.sample(5)

Xage = df.drop(['Gender','Annual Income (k$)'], axis = 1)

Xage.sample(5)

from sklearn.cluster import KMeans

K_means_model = KMeans(n_clusters=5)

K_means_model.fit(Xage)

K_means_model.predict(Xage)

K_means_model.fit_predict(Xage)

K_means_model.labels_

"""#### *ii. Determine optimal number of clusters*"""

from sklearn.neighbors import BallTree
import numpy as np
import pandas as pd
def hopkins(data_frame, sampling_size):
    """Assess the clusterability of a dataset. A score between 0 and 1, a score around 0.5 express
    no clusterability and a score tending to 0 express a high cluster tendency.
    Parameters
    ----------
    data_frame : numpy array
        The input dataset
    sampling_size : int
        The sampling size which is used to evaluate the number of DataFrame.
    Returns
    ---------------------
    score : float
        The hopkins score of the dataset (between 0 and 1)
    Examples
    --------
    >>> from sklearn import datasets
    >>> from pyclustertend import hopkins
    >>> X = datasets.load_iris().data
    >>> hopkins(X,150)
    0.16
    """
    if type(data_frame) == np.ndarray:
        data_frame = pd.DataFrame(data_frame)
    # Sample n observations from D : P
    if sampling_size > data_frame.shape[0]:
        raise Exception(
            'The number of sample of sample is bigger than the shape of D')
    data_frame_sample = data_frame.sample(n=sampling_size)
    # Get the distance to their neirest neighbors in D : X
    tree = BallTree(data_frame, leaf_size=2)
    dist, _ = tree.query(data_frame_sample, k=2)
    data_frame_sample_distances_to_nearest_neighbours = dist[:, 1]
    # Randomly simulate n points with the same variation as in D : Q.
    max_data_frame = data_frame.max()
    min_data_frame = data_frame.min()
    uniformly_selected_values_0 = np.random.uniform(min_data_frame[0], max_data_frame[0], sampling_size)
    uniformly_selected_values_1 = np.random.uniform(min_data_frame[1], max_data_frame[1], sampling_size)
    uniformly_selected_observations = np.column_stack((uniformly_selected_values_0, uniformly_selected_values_1))
    if len(max_data_frame) >= 2:
        for i in range(2, len(max_data_frame)):
            uniformly_selected_values_i = np.random.uniform(min_data_frame[i], max_data_frame[i], sampling_size)
            to_stack = (uniformly_selected_observations, uniformly_selected_values_i)
            uniformly_selected_observations = np.column_stack(to_stack)
    uniformly_selected_observations_df = pd.DataFrame(uniformly_selected_observations)
    # Get the distance to their neirest neighbors in D : Y
    tree = BallTree(data_frame, leaf_size=2)
    dist, _ = tree.query(uniformly_selected_observations_df, k=1)
    uniformly_df_distances_to_nearest_neighbours = dist
    # return the hopkins score
    x = sum(data_frame_sample_distances_to_nearest_neighbours)
    y = sum(uniformly_df_distances_to_nearest_neighbours)
    if x + y == 0:
        raise Exception('The denominator of the hopkins statistics is null')
    return x / (x + y)[0]

hopkins(Xage, Xage.shape[0])
## Sıfıra yakın clustering için uygun..

ssd = []

K = range(2,10)

for k in K:
    model = KMeans(n_clusters =k)
    model.fit(Xage)
    ssd.append(model.inertia_)

plt.plot(K, ssd, "bo--")
plt.xlabel("Different k values")
plt.ylabel("inertia-error") 
plt.title("elbow method")

data=df.iloc[:,[1,2]].values

kmeans=KMeans(n_clusters=5,init='k-means++',random_state=0)
y_kmeans=kmeans.fit_predict(data)

#plotting the the clusters
fig,ax = plt.subplots(figsize=(12,6))
ax.scatter(data[y_kmeans==0,0],data[y_kmeans==0,1],s=100,c='red',label='Cluster 1')
ax.scatter(data[y_kmeans==1,0],data[y_kmeans==1,1],s=100,c='blue',label='Cluster 2')
ax.scatter(data[y_kmeans==2,0],data[y_kmeans==2,1],s=100,c='green',label='Cluster 3')
ax.scatter(data[y_kmeans==3,0],data[y_kmeans==3,1],s=100,c='cyan',label='Cluster 4')
ax.scatter(data[y_kmeans==4,0],data[y_kmeans==4,1],s=100,c='magenta',label='Cluster 5')

ax.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=275,c='black',label='Centroid')
plt.title('Cluster Segmentation of Customers')
plt.xlabel('Annual Income(K$)')
plt.ylabel('Spending Score(1-100)')
plt.legend()
plt.show()

ssd

-pd.Series(ssd).diff()

from yellowbrick.cluster import KElbowVisualizer

model_ = KMeans()
visualizer = KElbowVisualizer(model_, k=(2,9))

visualizer.fit(Xage)        
visualizer.poof();

"""### Why silhouette_score is negative?"""

from sklearn.metrics import silhouette_score

K_means_model.labels_

silhouette_score(Xage, K_means_model.labels_)

range_n_clusters = range(2,9)
for num_clusters in range_n_clusters:
   
    kmeans = KMeans(n_clusters=num_clusters)
    kmeans.fit(Xage)
    cluster_labels = kmeans.labels_
    
    silhouette_avg = silhouette_score(Xage, cluster_labels)
    print(f"For n_clusters={num_clusters}, the silhouette score is {silhouette_avg}")

from yellowbrick.cluster import SilhouetteVisualizer

model3 = KMeans(n_clusters=4)          
visualizer = SilhouetteVisualizer(model3)

visualizer.fit(Xage)
visualizer.poof();

model3.labels_

visualizer.silhouette_samples_

labels_0 = (model3.labels_ == 0)
labels_0

labels_1 = (model3.labels_ == 1)
labels_1

labels_2 = (model3.labels_ == 2)
labels_2

labels_3 = (model3.labels_ == 3)
labels_3

mean_silhoutte_score_1 = visualizer.silhouette_samples_[labels_3].mean()
mean_silhoutte_score_1

mean_silhoutte_score_2 = visualizer.silhouette_samples_[labels_3].mean()
mean_silhoutte_score_2

#model2.n_clusters
for i in range(4):
    label = (model3.labels_== i)
    print(f"mean silhouette score for label {i:<4} : {visualizer.silhouette_samples_[label].mean()}")
print(f"mean silhouette score for all labels : {visualizer.silhouette_score_}")

"""![image.png](attachment:image.png)

silhouette_score = (b-a)/max(a,b)

b : the mean nearest-cluster distance 
a : the mean intra-cluster distance 

for red point, 

b = 1 
a = ((1+1)**0.5 + (1+1)**0.5)/2  ==> 1.41

silhouette_score = (1-1.41)/1.41 ==> -0.29

#### *iii. Apply K Means*
"""

model = KMeans(n_clusters = 4)
model.fit_predict(Xage)

"""#### *iv. Visualizing and Labeling All the Clusters*"""

clusters = model.labels_

Xage.sample(15)

Xage["predicted_clusters"] = clusters
Xage

Xage['Class'] = Xage['predicted_clusters'].replace([0,1,2,3], ['En Düşük', 'En Yüksek','Orta-Düşük', 'Orta-Yüksek'])

Xage.sample(10)

Hiarc = Xage

Hiarc

"""### Clustering based on Annual Income and Spending Score

#### *i. Create a new dataset with two variables of your choice*
"""

df.sample(3)

Xann = df.drop(['Age','Gender'], axis = 1)

Xann.sample(4)

"""#### *ii. Determine optimal number of clusters*"""

K_means_modelann = KMeans(n_clusters=5)

K_means_modelann.fit(Xann)

K_means_modelann.predict(Xann)

K_means_modelann.labels_

hopkins(Xann, Xann.shape[0])
## sıfıra yakın. K-means için uygun

ssd = []

K = range(2,11)

for k in K:
    model = KMeans(n_clusters =k)
    model.fit(Xann)
    ssd.append(model.inertia_)

plt.plot(K, ssd, "bo--")
plt.xlabel("Different k values")
plt.ylabel("inertia-error") 
plt.title("elbow method")

kmeans=KMeans(n_clusters=5,init='k-means++',random_state=0)
y_kmeans=kmeans.fit_predict(data)

#plotting the the clusters
fig,ax = plt.subplots(figsize=(12,6))
ax.scatter(data[y_kmeans==0,0],data[y_kmeans==0,1],s=100,c='red',label='Cluster 1')
ax.scatter(data[y_kmeans==1,0],data[y_kmeans==1,1],s=100,c='blue',label='Cluster 2')
ax.scatter(data[y_kmeans==2,0],data[y_kmeans==2,1],s=100,c='green',label='Cluster 3')
ax.scatter(data[y_kmeans==3,0],data[y_kmeans==3,1],s=100,c='cyan',label='Cluster 4')
ax.scatter(data[y_kmeans==4,0],data[y_kmeans==4,1],s=100,c='magenta',label='Cluster 5')

ax.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=275,c='black',label='Centroid')
plt.title('Cluster Segmentation of Customers')
plt.xlabel('Annual Income(K$)')
plt.ylabel('Spending Score(1-100)')
plt.legend()
plt.show()

ssd

-pd.Series(ssd).diff()

from yellowbrick.cluster import KElbowVisualizer

model_ = KMeans()
visualizer = KElbowVisualizer(model_, k=(2,11))

visualizer.fit(Xann)        
visualizer.poof();

silhouette_score(Xann, K_means_modelann.labels_)

range_n_clusters = range(2,9)
for num_clusters in range_n_clusters:
   
    kmeans = KMeans(n_clusters=num_clusters)
    kmeans.fit(Xann)
    cluster_labels = kmeans.labels_
    
    silhouette_avg = silhouette_score(Xann, cluster_labels)
    print(f"For n_clusters={num_clusters}, the silhouette score is {silhouette_avg}")

from yellowbrick.cluster import SilhouetteVisualizer

modelann = KMeans(n_clusters=5)          
visualizer = SilhouetteVisualizer(modelann)

visualizer.fit(Xann)
visualizer.poof();

#model2.n_clusters
for i in range(4):
    label = (modelann.labels_== i)
    print(f"mean silhouette score for label {i:<4} : {visualizer.silhouette_samples_[label].mean()}")
print(f"mean silhouette score for all labels : {visualizer.silhouette_score_}")

"""#### *iii. Apply K Means*"""

modelann = KMeans(n_clusters = 5)
modelann.fit_predict(Xann)

"""#### *iv. Visualizing and Labeling All the Clusters*"""

class_ann = modelann.labels_

Xann["Class_Ann"] = class_ann
Xann

plt.figure(figsize=(10,6))
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Class_Ann', data=Xann, palette="viridis")
centers = modelann.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=300, alpha=0.5);

"""### Hierarchical Clustering

### *i. Determine optimal number of clusters using Dendogram*

### Clustering based on Age and Spending Score- x1
"""



Xage = Xage.drop(["Class","predicted_clusters"], axis =1)

Xage_hc = Xage

Xage_hc

linkage_dict = {"hc_ward" : linkage(y = Xage_hc, method = "ward"),
                "hc_complete" : linkage(Xage_hc, "complete"),
                "hc_average" : linkage(Xage_hc, "average"),
                "hc_single" : linkage(Xage_hc, "single")}


plot_list = list(range(221,225))

plt.figure(figsize = (20,12))

for _plot, (title, method) in zip(plot_list, linkage_dict.items()):
    plt.subplot(_plot)
    plt.title(title)
    plt.xlabel("Observations")
    plt.ylabel("Distance")
    dendrogram(method,truncate_mode = "lastp", p = 10, leaf_font_size = 10)

from sklearn.cluster import AgglomerativeClustering

model1 = AgglomerativeClustering(n_clusters=4, affinity = "euclidean", linkage = "ward")
model1.fit_predict(Xage_hc)

Xage_hc["class_hc"] = model1.labels_
Xage_hc

Xage_hc.head(10)

plt.figure(figsize=(10,5))
plt.title("Cluster Count")
ax = sns.countplot(x = Xage.class_hc)
#ax.bar_label(ax.containers[0])
for bar in ax.patches:
    ax.annotate(format(bar.get_height(), '.0f'),
                   (bar.get_x() + bar.get_width() / 2,
                    bar.get_height()), ha='center', va='center',
                   size=12, xytext=(0, 8),
                   textcoords='offset points')
plt.show();

Xage_hc.groupby("class_hc")["Spending Score (1-100)"].mean()

K = range(2,8)

for k in K:
    model = AgglomerativeClustering(n_clusters = k)
    model.fit(Xage_hc)
    print(f'Silhouette Score for {k} clusters: {silhouette_score(Xage_hc, model.labels_)}')

print(f'Silhouette Score(n=3): {silhouette_score(Xage_hc, Xage_hc.class_hc)}')

Xage_hc["index_no"] = Xage_hc.index
Hiarc["index_no"] = Hiarc.index

Xage_hc.sample(3)

Hiarc.sample(3)

Xage['class_hc_label'] = Xage['class_hc'].replace([2,0,1,3], ['En Düşük', 'En Yüksek','Orta-Düşük', 'Orta-Yüksek'])

Xage = Xage.drop("class_hc", axis =1)

mrg_df = pd.merge(left = Xage, right = Hiarc, how = "inner")
mrg_df.drop("index_no", axis = 1, inplace= True)
mrg_df

mrg_df[mrg_df['class_hc_label'] == mrg_df['Class']]

"""## Clustering based on Annual Income and Spending Score- x2"""



"""### ii. *Apply Agglomerative Clustering*

#### Age and Spending Score- x1
"""



"""#### Annual Income and Spending Score- x2"""



"""### iii. *Visualizing and Labeling All the Clusters*

#### Age and Spending Score- x1
"""



"""#### Annual Income and Spending Score- x2"""



"""#### Interpretation based on Age and Spending Score- x1"""



"""### Conclusion

**cluster 0** : The average age is around 55, both annula_income and spending_scores are on average. 
It should be researched what can be done to direct to more spending.

**cluster 1**: The average age is around 45, the annula_income is high but the spending_scores are very low. 
This group is our target audience and specific strategies should be developed to drive this group to spend.

**cluster 2** :The average age is around 30. The annula_income is high and spending_scores are very high. 
This group consists of our loyal customers. Our company derives the main profit from this group. Very 
special promotions can be made in order not to miss it.    
    
**cluster 3**: The average age is around 25.both annula_income and spending_scores are on average. 
It should be researched what can be done to direct to more spending.

#### Interpretation based on Annual Income and Spending Score- x2
"""



"""## Conclusion

### Female

**cluster 0** : The average age is around 40, both annula_income and spending_scores are on average. 
It should be researched what can be done to direct more spending.

**cluster 1**: The average age is around 45, the annula_income is very high but the spending_scores is low.
This group is our target audience and special strategies need to be developed for this group.    

**cluster 2** :The average age is around 45. Both annula_income and spending_scores are low. It can be 
directed to shopping with gift certificates.

**cluster 3**: The average age is around 25. Low annual_incomes but very high spending scores. This 
group does a lot of shopping, but they do not bring much profit.

**cluster 4**: The average age is around 30, the annual income and the spending_score
is very high. This group consists of our loyal customers. Our company derives the main profit from this group. 
Very special promotions can be made in order not to miss it.

### Male

**cluster 0** : The average age is around 45, both annula_income and spending_scores are on average. 
It should be researched what can be done to direct more spending.

**cluster 1**: The average age is around 40, the annula_income is very high but the spending_scores is very low.
This group is our target audience and special strategies need to be developed for this group.    

**cluster 2** :The average age is around 50. Both annula_income and spending_scores are low. It can be 
directed to shopping with gift certificates.

**cluster 3**: The average age is around 25. Low annual_incomes but very high spending scores. This 
group does a lot of shopping, but they do not bring much profit.

**cluster 4**: The average age is around 30, the annual income and the spending_score
is very high. This group consists of our loyal customers. Our company derives the main profit from this group. 
Very special promotions can be made in order not to miss it.
"""



"""**cluster 0** : The average age is around 40, both annula_income and spending_scores are on average. 
It should be researched what can be done to direct more spending.

**cluster 1**: The average age is around 30, both annula_income and spending_scores are very high. 
This group consists of our loyal customers. Our company derives the main profit from this group. Very 
special promotions can be made in order not to miss it.

**cluster 2** :The average age is around 45. Both annula_income and spending_scores are low. It can be 
directed to shopping with gift certificates.

**cluster 3**: The average age is around 25. Low annual_incomes but very high spending scores. This 
group does a lot of shopping, but they do not bring much profit.

**cluster 4**: The average age is around 40, their annual income is very high but their spending_score
is very low. This group is our target audience and special strategies need to be developed for this 
group.

___

<p style="text-align: center;"><img src="https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV" class="img-fluid" alt="CLRSWY"></p>

___
"""